[["index.html", "Remote Sensing &amp; Humanitarian Assessment Analysis Welcome", " Remote Sensing &amp; Humanitarian Assessment Analysis Zack Arno &amp; Matt Wencel 2023-01-31 Welcome Welcome to Remote Sensing &amp; Humanitarian Assessment Analysis. Here we describe some of the current approaches to integrate remote sensing into humanitarian assessment data. While this book touches on Google Earth Engine, R, Python, and Remote Sensing generally it neither a beginners guide or a comprehensive look at any of those topics. This is an ongoing work and you can visit the books GitHub Repository for more details or if you want to contribute.It is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License The work discussed in this document is centralized in the Impact-Initiatives Geospatial GitHub Repository, with the majority being wrapped/packaged further into the {surveyGEER} package/sub repository. "],["introduction.html", "1 Introduction 1.1 Overview 1.2 Current State of Work", " 1 Introduction 1.1 Overview This document attempts to describe some of the global approaches done by IMPACT-Initiatives to integrate remote sensing (RS) techniques into analyses and products. IMPACT/REACH has been rapidly expanding its RS portfolio and various country teams are increasingly integrating RS into their products/ &amp; analyses. This document does not dive into all of the work being done by individual country teams, but rather focuses on global approaches and, in particular, the integration of RS into analysis of humanitarian needs assessments. The book describes a methodology developed for integrating remote sensing into humanitarian survey/assessment analysis and serves as a repository for use-cases, tutorials, templates, and methods. Chapters 1-5 focus on the methodology, while the rest showcase use-cases. {surveyGEER} is a project that was developed as an R package which hosts a variety of tools, templates, and even data processing pipelines. It was named after the initial intent to create methods to integrate analyses of HH survey data with remote sensing using Google Earth Engine (GEE) remote sensing in R. In actuality the package does not contain any processes or functions for analyzing complex survey data, but rather focuses on the remote sensing component and methods to bring the remote sensing sources directly into HH data for downstream analysis. 1.2 Current State of Work 1.2.1 Bringing RS into humanitarian assessments A key objective of this work was to explore a methodology for integrating remote sensing into assessment data sets. A methodology was developed for both household and key informant (KI) level assessments. However, house hold level was the primary focus of this work. Remote sensing derived metrics/indicators were extracted into the the data sets of 6 REACH Multi-Sectoral Needs Assessments (MSNAs) and 4 KI level assessments that were part of the Humanitarian Situation Monitoring (HSM) validation study. The assessments included: MSNAs Colombia (COL) Haiti (HTI) Iraq (IRQ) Niger (NER) Nigeria (NGA) Somalia (SOM) HSM South Sudan (SSD) Burkina Faso (BFA) Democratic Republic of Congo (DRC) Central African Republic (CAR) The tabular MSNA assessment data sets with chosen remote sensing derived indicators were provided back to the country teams to integrate into subsequent contextual analyses along with an extraction report and code book to simplify and explain the remote sensing variables. The HSM data sets were shared with HQ analysts. Once the data and explanatory extraction report was provided, the country teams with the contextual expertise were given the responsibility of analyzing the provided environmental/climatic data against humanitarian conditions captured via field surveys. In the case of HSM data, HQ data analysts in the HSM department were given this responsibility. At the time of writing, regarding the MSNA analysis, little country level resources have been allocated for this step. Colombia appears to be the only country thus far to have begun this analysis and the results look promising. To ensure that some data exploration was performed the authors (HQ remote sensing department) performed some preliminary exploratory analysis of the NGA data. These results are presented in chapter 5. Regarding the HSM validation study, the HSM analysts have agreed to integrate the tabular remote sensing data into there analyses with a primary focus on South Sudan. The lower sample sizes an reduced spatial coverage of remaining 3 HSM data sets warrant de-prioritization. 1.2.2 Case Studies/Tutorials The case studies/tutorials presented in this book just represent a few interesting applications of remote sensing in humanitarian work. Some of the methods presented were used in the above assessment methodology (chapter 1-5) while others were used for ad-hoc analysis, fact sheets, maps, and other products. This section contains only a fraction of the use-cases developed so far and contains basically none of the country-developed work flows. It is the authors hope that country teams would be able to immediately contribute to this section so that it might serve as some sort of repository and reference for a lot of the great work that has already been done. "],["methodology.html", "2 Methodology 2.1 Overview of Approach 2.2 Method 2.3 Data sharing/coordinate anonymization procedure 2.4 Data Extraction 2.5 Extraction Results and Sharing", " 2 Methodology 2.1 Overview of Approach Remote sensing data is rich both temporally and spatially, but often there is no rationale to aggregate it to administrative boundaries. Figure 2.1 below shows NDWI across various admin 2 boundaries in SSD. The data clearly mirrors different eco-zones/environmental phenomena which change at fine spatial scales. By aggregating to a single point estimate per admin unit, the data becomes difficult to interpret. There is little rationale to hypothesize that a HHs condition would correlate to single NDWI point estimate for the region. However, it is intuitive to assume that flooding in the HHs immediate surroundings might impact their welfare. Figure 2.1: NDWI calculated around the white nile in South Sudan. Points Represent theoretical household locations The importance of spatially explicit HH data to understand needs has been well documented as significant and important village-wise differences in mortality and nutritional outcomes have been shown to occur in the same locality (Tomkins et al., 1978). Mapped drivers such as drinking water supply were shown to explain significant differences in mortality rates amongst villages during the Darfur famine (De Waal, 1989). 2.2 Method The goal of the data pipeline for survey data was to pull environmental/climatic variables derived from remote sensing directly in the survey data set for subsequent analysis against humanitarian conditions/needs. The process can be simplified as 3 steps: coordinate anonymization RS indicator construction and extraction to coordinate points Integration of extracted variables back to clean data set 2.3 Data sharing/coordinate anonymization procedure Spatial coordinates are included with surveys to enable and ensure quality control of assessment data. However, this data is not published or shared as it is considered sensitive information which could be used to re-identify households. However, for the remote sensing analysis/data extraction the coordinates were required. Therefore, to ensure appropriate handling of sensitive data a coordinate anonymization step was performed prior to the country team sharing the data. The process of anonymization is performed in the workflow.rmd file in the vignettes folder of the repository. Essentially the analyst loads in the data and a new random universally unique identifier (uuid) is added to the data set. The uid and the coordinates are then split into a new file contained in the data_share folder which was then shared with the HQ RS analyst. At the same time a lookup table containing the new old uuid and new uuid is automatically saved to the vault directory. After the file in the data_share folder is shared with the HQ analyst all of the RS extraction is run on the anonymized coordinates. Once the data has been extracted to the anonymized coordinates the RS analyst sends the new file with RS parameters back to the country office who then loads that file into the second half of the workflow.rmd and joins the RS coordinates back to the clean data via the uuid lookup table. 2.4 Data Extraction Selected generic RS indicators were prioritized for extraction (Appendix x). These were then modified per context based on the timing of the assessment and information regarding dates/seasons that affect agriculture/livelihoods that was provided by the country teams. Generic functions were built using the Google Earth Engine via the {rgee} API and {tidyrgee} package to extract indicators of interest with flexible input parameters. These generic functions can potentially be useful in external applications. The targets workflow package was used to create a pipeline to perform the data extraction. Using this framework the analyst sets up a series of targets or function commands to do various operations, the results of these operations are saved and organized within the repository. Each time the pipeline is executed all the targets and source code is logged so that it never has to be run again if the source code is unchanged. This functionality is invaluable when running heavy data science operations for which you want a transparent and reproducible code base. As these extractions were quite heavy (on the scale of hours) this work would not have been possible to scale across so many countries without the targets work flow. 2.5 Extraction Results and Sharing After the RS data was extracted to each coordinate the data set was shared back to the country office to be joined back to the data set using the surveyGEER workflow.rmd. In addition to a tabular data set and RS extraction report was shared with each team. The report included displays of assessment spatial coverage, explanations of the indicators extracted histogram/density plots of each of the indicators "],["rs-extraction-pipeline-example.html", "3 RS Extraction Pipeline Example 3.1 Targets 3.2 Example", " 3 RS Extraction Pipeline Example 3.1 Targets explain what {targets} is and how it heleps 3.2 Example using colombia code as a placeholder to demonstrate the targets pipeline, but can choose a better one. # Created by use_targets(). # Follow the comments below to fill in this target script. # Then follow the manual to check and run the pipeline: # https://books.ropensci.org/targets/walkthrough.html#inspect-the-pipeline # nolint # Load packages required to define the pipeline: library(targets) library(rgee) library(rlang) library(dplyr) rgee::ee_Initialize(drive=T) # library(tarchetypes) # Load other packages as needed. # nolint # Set target options: # tar_option_set(envir= getNamespace(&quot;surveyGEER&quot;)) tar_option_set( packages = c(&quot;tidyverse&quot;, &quot;rgee&quot;, &quot;lubridate&quot;, # &quot;rstudioapi&quot;, &quot;here&quot;, &quot;tidyrgee&quot;, &quot;sf&quot; ), # imports = &quot;surveyGEER&quot;, # envir = getNamespace(&quot;surveyGEER&quot;), # packages that your targets need to run format = &quot;rds&quot; # default storage format # Set other options as needed. ) # tar_make_clustermq() configuration (okay to leave alone): options(clustermq.scheduler = &quot;multiprocess&quot;) # tar_make_future() configuration (okay to leave alone): # Install packages {{future}}, {{future.callr}}, and {{future.batchtools}} to allow use_targets() to configure tar_make_future() options. # Load the R scripts with your custom functions: lapply(list.files(&quot;R&quot;, full.names = TRUE, recursive = TRUE), source) # source(&quot;other_functions.R&quot;) # Source other scripts as needed. # nolint # cntry_code &lt;- c(&quot;col&quot;) # Replace the target list below with your own: # Colombia Targets list( # Colombia ---------------------------------------------------------------- tar_target( name = col_pt_data_clean, command = load_clean_col_assessement_points(country_code = &quot;col&quot;) ), tar_target( name=col_oxford_access, command= extract_oxford_access_indicators(geom_sf = col_pt_data_clean,img_scale = 928) ), tar_target( name= col_landforms, command = extract_geomorph_landform_indicators(col_pt_data_clean,img_scale=90) ), tar_target( name= col_landforms_reclassified, command= recode_srtm_alos_categorical(df = col_landforms) ), tar_target( name= col_chirps_rainfall_intensity, command= extract_chirps_rain_intensity(geom_sf=col_pt_data_clean,from_when=&quot;2022-05-31&quot;) ), tar_target( name= col_chirps_rainfall_intensity_prepped, command= prep_rs_chirps_intensity_target(col_chirps_rainfall_intensity,moi=5) ), tar_target( name= col_chirps_spi, command= extract_spi_to_values(geom_sf=col_pt_data_clean,moi=5) ), tar_target( name= col_npp, command= extract_npp_indicators(geom_sf = col_pt_data_clean,img_scale = 500) ), tar_target( # need to build in range to composte over for colombia... 1 month == too cloudy....3 months still 40 % clouds # might be worth looking at 6 months and a year. name= col_air_quality, command= extract_s5p_air_quality(geom_sf = col_pt_data_clean,yoi=2022, moi=5, img_scale=111320) ), tar_target( name= col_dist_to_coast, command= extract_dist_to_coast(geom_sf=col_pt_data_clean,country_code = &quot;col&quot;,pt_density = 100) ), tar_target( name = col_prev_3mo_drought_modis_basea, command = extract_monthly_modis_drought(geom_sf=col_pt_data_clean, baseline_years = c(2000:2015), moi = c(3, 4, 5), yoi = c(2022), scale = 250, mask = &quot;cloud&amp;quality&quot;, satellite = &quot;terra&quot;, TAC = T, temporal_interpolation = T) ), tar_target( name= col_prev_3mo_drought_modis_basea_prepped, command = prep_rs_modis_target(col_prev_3mo_drought_modis_basea) ), tar_target(name= col_local_value, command=extract_local_values_to_points(schema = &quot;col&quot;, country_code = &quot;col&quot;, geom_sf = col_pt_data_clean) ), tar_target( name= col_local_value_merged, command= merge_local_layers(col_local_value) ), tar_target( name = col_rs_indicators_long, command= format_rs_indicators_long(country_code = &quot;col&quot;, col_pt_data_clean, col_chirps_rainfall_intensity_prepped, col_prev_3mo_drought_modis_basea_prepped, col_chirps_spi, col_dist_to_coast, col_landforms_reclassified, col_oxford_access, col_npp,col_air_quality, col_local_value_merged ) ), tar_target( name = col_rs_indicators_wide, command= format_rs_indicators_wide(col_rs_indicators_long) ) ) "],["remote-sensing-extraction-report-nga.html", "4 Remote Sensing Extraction Report (NGA) 4.1 Intro 4.2 Overview map 4.3 RS Quality Isses 4.4 Remote Sensing Indicators", " 4 Remote Sensing Extraction Report (NGA) 4.1 Intro This chapter is just and example of one of the Preliminary RS Extraction Reports that was provided to the country offices in addition to the RS data in tabular format (1 row per HH). The format has been adapted slightly to fit into this book format. The objective of the report is to help the country team analysts understand how to interpret the RS indicators. After reading through the report the country team analyst should analyze the RS indicators provided against relevant survey variables/indicators. As the RS indicators have been added to the HH data as new columns with the rs_ prefix analysis can be carried out with the same tools as typical survey analysis (i.e {survey},{srvyr},{hypegrammar},{butteR}). The analyst should explore indicators in the data set that might be effected by climatic, environmental, or geographic drivers. At the end of the report we offer some suggestions which might make more sense after reviewing the RS variables here. 4.2 Overview map Below is the spatial distribution of the interviews carried out. 4.1 Figure 4.1: Overview map of NGA assessment coverage 4.3 RS Quality Isses A brief summary of quality issues in the RS extraction is provided. 4.3.1 Numeric RS variables Below a table 4.1 shows the % of values that returned null/NA values for numeric indicators. NPP is the highest with 12.4 % NA. This is likely due to cloud cover issues. Recommend leaving the NA values in the data set and excluding from subsequent analysis All the rest had no NA values Table 4.1: % NA per umeric indicator name count count_na % NA NPP 10774 1341 12.4 MODIS April (VCI + NDVI) 10774 31 0.3 MODIS March (VCI + NDVI) 10774 31 0.3 Accessibility 10774 0 0.0 Air quality 10774 0 0.0 MODIS May (VCI + NDVI) 10774 0 0.0 Rainfall intensity 10774 0 0.0 rs_ndvi_z_growing_season_21 10774 0 0.0 SPI 10774 0 0.0 4.3.2 Categorical RS Variables Below a table shows the % of values that returned null/NA values for categorical indicators. Other than the land form classifications the rest of the data was obtained from auxiliary data sources provided by the team. auxiliary data set. The only auxiliary data set was the livelihood zones vector file. The NA values are likely due to edge effects or topology issues in the file. recommend leaving these values as NA and not including in subsequent analysis of these variables category count count_na % NA Landforms 10774 0 0 Livelihood Zones 10774 5 0 4.4 Remote Sensing Indicators 4.4.1 Standard Precipitation Index (SPI) SPI (McKee et al. 1993) (McKee 1995) is calculated from chirps rainfall data (Funk et al. 2015) It is an widely-used indicator to characterize drought at multiple time scales. Here we have calculated it at 1, 3, 6, 9 , and 12 months (i.e spi1, spi3, etc) Below I have copied directly from Indianas Department of Natural Resources Web Page Interpretation of SPI at different time scales A 1-month SPI typically compares well to the percent of normal precipitation for the month. However, as with other SPI time scales, it is actually considered to be a more accurate representation of monthly precipitation for a given location because the long-term precipitation record (over 30 years or more) is fitted to a probability distribution. It is then transformed into a normal distribution so that the median SPI for the location and period is zero; that is, half of the historical precipitation amounts are below the median and half are above the median. Positive SPI values indicate greater than median precipitation (i.e. wet conditions), and negative values indicate less than median precipitation (i.e. dry conditions). The 1-month SPI is a short-term value and during the growing season can be important for correlation of soil moisture and crop stress. The 3-month SPI provides a comparison of the precipitation over a specific 3-month period with the precipitation totals from the same 3-month period for all the years included in the historical record. In other words, a 3-month SPI at the end of February compares the December-January-February precipitation total in that particular year with the December-February precipitation totals of all the years. A 3-month SPI reflects short- and medium-term moisture conditions and provides a seasonal estimation of precipitation. It is important to compare the 3-month SPI with longer time scales. A relatively normal 3-month period could occur in the middle of a longer-term drought that would only be visible at longer time scales. Looking at longer time scales would prevent a misinterpretation that any drought might be over. The 6-month SPI compares the precipitation for that period with the same 6-month period over the historical record. The 6-month SPI indicates medium-term trends in precipitation and is still considered to be more sensitive to conditions at this scale than the Palmer Index. A 6-month SPI can be very effective showing the precipitation over distinct seasons. Information from a 6-month SPI may also begin to be associated with anomalous streamflows and reservoir levels. The 9-month SPI provides an indication of precipitation patterns over a medium time scale. Droughts usually take a season or more to develop. SPI values below -1.5 for these time scales are usually a good indication that fairly significant impacts are occurring in agriculture and may be showing up in other sectors as well. Some regions of the country may find that the pattern displayed by the map of the Palmer Index closely relates to the 9-month SPI maps. For other areas, the Palmer Index is more closely related to the 12-month SPI. The Palmer Index maps are updated each week, although the patterns usually do not change significantly. The SPI maps are updated at the end of each month. A 12-month SPI is a comparison of the precipitation for 12 consecutive months with the same 12 consecutive months during all the previous years of available data. The SPI at these time scales reflect long-term precipitation patterns. Because these time scales are the cumulative result of shorter periods that may be above or below normal, the longer SPIs tend toward zero unless a specific trend is taking place. Chart Interpretation: these are histograms - the x-axis represent the SPI score for each of the specified time ranges (i.e spi1,spi3, spi6) the suffix number on the end of the chart title represents the number of months the SPI was calculated over. Another way think about it : rs_May_spi3 is a comparison of this years March-April-May rainfall compared to all the years on record. Therefore values &gt; 0 indicators more rain during that 3 month time period than normal and values &lt; 0 indicatore less rainfall than normal For SPI the historical baseline is calculated as all years on record (1981-present). This seems to be the standard method for the SPI calculations. 4.4.2 Maximum Rainfall Events These charts display the maximum rain fall events for different rainfall duration over different time periods (from daily CHIRPS measurements (Funk et al. 2015). We have chosen to calculate Maximum 3 day, 5 day, and 10 day rainfall over 30, 60, and 90 days. To break it down further: all calculations were started May 31, for 30 day calculations we looked at the period from 01 May - May 31 and calculated and found the 3 day consecutive period with the most rainfall in that 30 day window. Chart Interpretation: these are histograms - the x-axis represent the mm that fell in the maximum precip event and the y-axis is how many observations there were If you read across the charts horizontally the maximum event window changes from 3,5,10 days while the number of days that was looked at remains constant. Clearly the total mm of precipitation that fell over a 10 day precipitation record should tend to be more than those that fell over 3 days. If you read across vertically the maximum rain event stays constant while the number of days increases as you move down. Naturally if you increase the window for these calculations the number of mm should also increase. By combining rainfall amount (mm) with time we get a measurement of rainfall intensity. High intensity events might correlate with flooding indicators. 4.4.3 MODIS Vegetation Indicators MODIS 16 day (250 m) composites were used as the basis for these analyses. NDVI Z Score (or NDVI Standard Score): # of deviations from normal based on the historical record. VCI (vegetation condition index): Median percent normal (not shown) 4.4.3.1 NDVI Z score - Pre Assessment Lets take a look at the distribution of Z scores. Any values above 0 indicate locations where the vegetation appears healthier than normal (2000-2015) for that particular month. Values below zero indicate less healthy than normal conditions. This indicator shows a very nice normal distribution and is prime candidate for logistic regression. Should look in more detail outliers which might need to be treated or excluded. Box-plots give another view of the distribution and make it easier to see these outlier values. Should treat these outliers with care, probably it makes sense to set a standard deviation threshold at which point to delete the outliers (set them to NA) for subsequent analysis. Outliers might be due to cloud issue (most likely in the more recent imagery) 4.4.4 NDVI Z-Score - Previous Growing Season Approximate main growing season dates were provided by the team: 20 June - 26 September. Average NDVI was calculated over this time period at the pixel level for 2021. Standard deviation and mean/medians were also calculated over this time period/season throughout the the entire historical MODIS record including this season (2000-2021). With this information the NDVI Z score for the last major growing season prior to the assessment (2021) was calculated. Note: setting baselines is a bit subjective and there is no standard. Above we used 2000-2015 as a baseline while here we used 2000-2021 (historical start to time of interest). The framework for setting the baseline here is probably the one I have most commonly seen, but there are pros and cons of any of the methods. For now, this remains experimental and we can adjust baselines. However, the baseline should not have a huge affect on any major trends found. We recommend looking at this indicator with respect to food security, livelihoods, and any agricultural related indicators/questions. Below distribution of the average NDVI Z score over the 2021 major growing season. 4.4.5 Vegetation Conditon Index (VCI) VCI is another vegetation index that is often used as a drought indicator. More information can be found here Below is a table which shows a common classification for VCI values (Ghaleb, Mario, and Sandra 2015). I believe these classifications are based on the premise that the baseline/historical record includes the entire record up to the current time. Here the baseline is is only up to 2015. We may want to run again with the entire record. Nonetheless, the findings should not change a huge amount and relationships should still be investigated under this framework first Drought Values Extreme &lt;10 Severe &lt;20 Moderate &lt;30 Mild &lt;40 No &gt;=40 Below is the distribution of VCI for the 4.4.6 Landform Geomorphology Below is the % distribution of your data for each landform category. The landforms were extracted from two different data sources: ALOS &amp; SRTM (Theobald et al. 2015). They are compared here to understand consistency. You can experiment with either. Do valleys show different results than slopes/ridges? 4.4.7 Accessibliity Below are the metrics on accessibility. They are all put together by oxford (Weiss et al. 2018) (Weiss et al. 2020). - Same research group made all 3 metrics, would recommend the 2018 (Weiss et al. 2020) metrics as they are updates. 4.4.8 Auxiliary data sets Below are plots of the auxiliary data provided by the team. This interpretation is up to the office team. Below is the distribution of livelihood zones where the data was collected. References "],["preliminary-results.html", "5 Preliminary results 5.1 Overview 5.2 Nigeria 5.3 Colombia 5.4 Methodology", " 5 Preliminary results This chapter will walk through some of the initial exploratory analysis performed so far 5.1 Overview Below we show an overview map of the assessment activities carried out in Somalia, Nigeria, Iraq, and Colombia. Not going to give an overview of each countries climate yet 5.2 Nigeria Before exploring how remote sensing derived indicators might impact or relate to household indicators/questions we have to first understand what household indicators are most useful and representative of needs/conditions at the household level. Additionally, to relate the indicators designed in the assessment research design to environmental indicators we do need clear hypotheses of why/how an environmental condition could effect these measurements at the household level. This is difficult for an external analyst and is discussed further in the limitation/discussion section. 5.2.1 Indicators/hypotheses Going into this exercise we understood that generally food security, income/livelihoods, and potentially health related indicators would be the most opportune ares to relate to environmental indicators. Unfortunately the NGA data is light on food security (see limitations) therefore we shift towards income and start with some research questions and go on to break these into sub-research questions with hypothes: overall income for the HH was collected as a categorical ordinal question. Can we identify major drivers of income dsalkfadsk 5.2.2 Income Exploration Before attempting correlation/association with environmental variables we have to figure out if there are any other major drivers of income that we need to account for. If we plot estimate income by gender HoH (Figure ??) it becomes evident that male headed households dominate the higher income brackets and female the lower income brackets. nga_svy$data_main |&gt; group_by(gender_hoh,overall_income_estimate) |&gt; summarise( survey_mean(vartype = &quot;ci&quot;) ) |&gt; ggplot(aes(x= overall_income_estimate, y=coef,color=gender_hoh))+ scale_y_continuous(labels = scales::percent)+ geom_errorbar(aes(ymin=`_low`, ymax=`_upp`))+ labs(x= &quot;HH Income Estimate&quot;)+ geom_point()+ coord_flip()+ theme_bw()+ theme( axis.title.x = element_blank() ) Since we see this discrepancy lets look at the sample size quickly for nga_svy$data_main$variables |&gt; group_by(gender_hoh) |&gt; summarise( n=n() ) |&gt; knitr::kable() gender_hoh n male 10234 female 856 Since we know income is important lets have quick look Based on guidance from the country office we used MODIS data to extract the median NDVI anomaly around all for the previous 2021 growing season (June 20 - September 26). If we plot those values against income (just male hoh for now) we do not see a clear trend. We just see that all HHs generally tend to have slight negative Z-score for NDVI in the previous growing season. nga_svy$data_main |&gt; group_by(gender_hoh,overall_income_estimate) |&gt; summarise( survey_mean(rs_ndvi_z_growing_season_21,vartype = &quot;ci&quot;,na.rm=T), bla=unweighted(n()) ) |&gt; filter(gender_hoh==&quot;male&quot;) |&gt; ggplot(aes(x= overall_income_estimate, y=coef, color=gender_hoh ))+ # scale_y_continuous(labels = scales::percent)+ geom_errorbar(aes(ymin=`_low`, ymax=`_upp`))+ labs(x= &quot;HH Income Estimate&quot;)+ geom_point()+ coord_flip()+ theme_bw()+ theme( axis.title.x = element_blank() ) This brings us to a large question mark on how we should aggregate RS data for HH analysis. The MODIS data we are using is 250 m therefore the pixels extracted at the HH do represent an area larger than the HH and could theoretically include small shareholder farming around the HH. However: is 250 m sufficient to understand how agriculture/vegetative health in the area surrounding the HH could affect the HHs? Also I did manually scan satellite imagery in NW NGA in combination with the ESA global landcover and what became obvious is that while towns are rural they have a clear footprint. This means that aggregating hh points in the time rural towns would largely be influence by this cleared footprint. Additionally since we are calculating NDVI anomaly using all years on up to date of interest (2000-2021) HHs in new settlements and or settlements that have grown or become more urbanized in any way will be biased towards a negative ndvi anomaly signature Therefore we extract the same data, but this time using the ESA urban mask as described in chapter xxx. In summary, we mask the urban area and the apply a focal median to fill in the masked area. First we try it with a focal window size of 4 pixels (1000 meters) nga_svy$data_main |&gt; group_by(gender_hoh,overall_income_estimate) |&gt; summarise( survey_mean(rs_ndvi_z_growing_season_21_fs4,vartype = &quot;ci&quot;,na.rm=T), bla=unweighted(n()) ) |&gt; filter(gender_hoh==&quot;male&quot;) |&gt; ggplot(aes(x= overall_income_estimate, y=coef, color=gender_hoh ))+ # scale_y_continuous(labels = scales::percent)+ geom_errorbar(aes(ymin=`_low`, ymax=`_upp`))+ labs(x= &quot;HH Income Estimate&quot;)+ geom_point()+ coord_flip()+ theme_bw()+ theme( axis.title.x = element_blank() ) and then a focal window size of 10 pixels or 2.5 km (Figure ??). This exercise seems quite insightful as the expected trend of higher income associated with healthier vegetation compared to normal. You can see that when the focal window was increased the anomaly values become less negative. This could very likely be do to the larger window minimizing the effect of urbanized pixels. Now we must ask ourselves why are do the anomalies appear to be generally negative? is it because it is a drier than normal year over the entire study area? or is it potentially because of this urban affect on the anomalies. The increase in signature associated with increasing the window size suggests this is a possibility. It is quite likely that the ESA urban mask does not capture the full extent of the urban area in these rural towns. In additional to playing with the windows size a urban mask dilation procedure prior to masking should be explored - I hypothesize that the anomalies will shift more to more positive values and this relationship will also strengthen nga_svy$data_main |&gt; group_by(gender_hoh,overall_income_estimate) |&gt; summarise( survey_mean(rs_ndvi_z_growing_season_21_fs10,vartype = &quot;ci&quot;,na.rm=T), bla=unweighted(n()) ) |&gt; filter(gender_hoh==&quot;male&quot;) |&gt; ggplot(aes(x= overall_income_estimate, y=coef, color=gender_hoh ))+ # scale_y_continuous(labels = scales::percent)+ geom_errorbar(aes(ymin=`_low`, ymax=`_upp`))+ labs(x= &quot;HH Income Estimate&quot;)+ geom_point()+ coord_flip()+ theme_bw()+ theme( axis.title.x = element_blank() ) Now lets look at rainfall. # nga_svy$data_main$variables$rs_rx10d_90d_may nga_svy$data_main |&gt; group_by(gender_hoh,overall_income_estimate) |&gt; summarise( survey_mean(rs_rx10d_90d_may,vartype = &quot;ci&quot;,na.rm=T), bla=unweighted(n()) ) |&gt; filter(gender_hoh==&quot;male&quot;) |&gt; ggplot(aes(x= overall_income_estimate, y=coef, color=gender_hoh ))+ # scale_y_continuous(labels = scales::percent)+ geom_errorbar(aes(ymin=`_low`, ymax=`_upp`))+ labs(x= &quot;HH Income Estimate&quot;)+ geom_point()+ coord_flip()+ theme_bw()+ theme( axis.title.x = element_blank() ) nga_svy$data_main |&gt; group_by(overall_income_estimate,gender_hoh,i.have_cattle=`hh_situation/catte_rearing`) |&gt; summarise( pct_mean = survey_mean() ) |&gt; # filter(i.have_cattle==T) |&gt; ggplot(aes(x= overall_income_estimate, y=pct_mean ))+ geom_line(aes(group=overall_income_estimate) )+ geom_point(aes(color=gender_hoh),stat=&quot;identity&quot;,position= &quot;dodge&quot;)+ facet_wrap(~i.have_cattle,scales = &quot;free&quot;)+ coord_flip()+ theme_bw() Does having cattle affect reported income bracket? For female headed households it looks like the answer is no. For male headed households we see that the higher income brackets tend to be more likely to own cattle, whereas the lower income brackets do not. nga_svy$data_main |&gt; # filter(gender_hoh==&quot;male&quot;) |&gt; group_by(gender_hoh,i.have_cattle=`hh_situation/catte_rearing`,overall_income_estimate) |&gt; summarise( pct_mean = survey_mean() ) |&gt; # filter(i.have_cattle==T) |&gt; ggplot(aes(x= overall_income_estimate, y=pct_mean ))+ geom_line(aes(group=overall_income_estimate) )+ geom_point(aes(color=i.have_cattle),stat=&quot;identity&quot;,position= &quot;dodge&quot;)+ scale_y_continuous(labels = scales::percent)+ facet_wrap(~gender_hoh,scales = &quot;free&quot;)+ coord_flip()+ theme_bw() nga_svy$data_main |&gt; # filter(gender_hoh==&quot;male&quot;) |&gt; group_by(gender_hoh,rs_alos_landforms,overall_income_estimate) |&gt; summarise( pct_mean = survey_mean() ) |&gt; # filter(i.have_cattle==T) |&gt; ggplot(aes(x= overall_income_estimate, y=pct_mean ))+ # geom_line(aes(group=overall_income_estimate) )+ # geom_point(aes(color=i.have_cattle),stat=&quot;identity&quot;,position= &quot;dodge&quot;)+ geom_bar(aes(fill=rs_alos_landforms),stat=&quot;identity&quot;,position= &quot;dodge&quot;)+ scale_y_continuous(labels = scales::percent)+ facet_wrap(~gender_hoh,scales = &quot;free&quot;)+ coord_flip()+ theme_bw() nga_svy$data_main |&gt; # filter(gender_hoh==&quot;male&quot;) |&gt; group_by(gender_hoh,overall_income_estimate,rs_alos_landforms) |&gt; summarise( pct_mean = survey_mean() ) |&gt; # filter(i.have_cattle==T) |&gt; ggplot(aes(x= overall_income_estimate, y=pct_mean ))+ # geom_line(aes(group=overall_income_estimate) )+ # geom_point(aes(color=i.have_cattle),stat=&quot;identity&quot;,position= &quot;dodge&quot;)+ geom_bar(aes(fill=rs_alos_landforms),stat=&quot;identity&quot;,position= &quot;dodge&quot;)+ scale_y_continuous(labels = scales::percent)+ facet_wrap(~gender_hoh,scales = &quot;free&quot;)+ coord_flip()+ theme_bw() 5.2.3 Accessibility Oxford Malaria project produced a global RS dataset in 2019 that predicts walking and total travel times to nearest health center. Here we test it against HH reports of travel times to nearest health center. nga_svy$data_main |&gt; group_by(nearest_health_care) |&gt; summarise( pct_mean = survey_mean(rs_healthcare_accessbility_walking_only2019,na.rm=T,vartype=&quot;ci&quot;), n_unweighted=unweighted(n()) ) |&gt; ggplot(aes(x= nearest_health_care, y=pct_mean ))+ geom_point()+ geom_errorbar(aes(ymin=pct_mean_low, ymax= pct_mean_upp, width=0.2))+ geom_line(group=1)+ geom_text(aes(label=n_unweighted,x=nearest_health_care,y=23, fill=NULL)) + scale_y_continuous()+ labs(x=&quot;Reported Travel Time&quot;, y=&quot;Remotely sensed walking time to healthcare facility (minutes)&quot;, title = &quot;Do reported walking travel times to nearest healthcare facility correlate with remotely sensed travel time?&quot;, subtitle = &quot;Northern Nigeria MSNA 2022&quot;, caption = str_wrap(&quot;We see that the remotely sensed walking times to nearest health center do generally correlate with HH reported travel times. However, RS calculations for HHs that reported over 15 minutes travel time do appear to be too low&quot;,180) )+ theme_bw()+ theme( plot.caption = element_text(hjust = 0), ) Does the distance corroborate with accessibility indicators? yes it does, but results are under estimated nga_svy$data_main |&gt; mutate( i.nearest_healthcare_barrier = case_when( nearest_health_care_adult %in% c(&quot;no_barriers&quot;,&quot;did_not_need&quot;)~&quot;No Barrier&quot;, nearest_health_care_adult == &quot;no_facility_nearby&quot;~&quot;Too Far&quot;, TRUE~NA_character_ ) ) |&gt; filter(!is.na(i.nearest_healthcare_barrier)) |&gt; group_by(i.nearest_healthcare_barrier) |&gt; summarise( pct_mean = survey_mean(rs_healthcare_accessbility_walking_only2019,na.rm=T,vartype=&quot;ci&quot;), n_unweighted=unweighted(n()) ) |&gt; ggplot(aes(x= i.nearest_healthcare_barrier, y=pct_mean ))+ geom_point()+ geom_errorbar(aes(ymin=pct_mean_low, ymax= pct_mean_upp, width=0.2))+ # geom_line(group=1)+ geom_text(aes(label=n_unweighted,x=i.nearest_healthcare_barrier,y=23, fill=NULL)) + scale_y_continuous()+ labs(x=&quot;Barrier to accessing facility?&quot;, y=&quot;Remotely sensed walking time to healthcare facility (minutes)&quot;, title = &quot;RS does indicate farther walking times for those reporting distance as a barrier to accessing healthcare&quot;, subtitle = &quot;Northern Nigeria MSNA 2022&quot;, caption = str_wrap(&quot;RS does corraborate, but the distances do appear underestimated&quot;,180) )+ theme_bw()+ theme( plot.caption = element_text(hjust = 0), ) 5.2.4 MSNI It looks like increased rainfall intensify associated with increased MSNI needs, but there is weird spatial element. nga_svy$data_main |&gt; group_by(msni=as_factor(msni)) |&gt; summarise( mean_pct = survey_mean(rs_rx5d_90d_may,na.rm=T,vartype = &quot;ci&quot;) ) |&gt; ggplot(aes(x= msni, y=mean_pct ))+ geom_point(stat=&quot;identity&quot;)+ geom_line(group=1)+ geom_errorbar( aes(ymin = `mean_pct_low`, ymax = `mean_pct_upp`), width = 0.2)+ # geom_text(aes(label=n,x=variable_val ,y=20, fill=NULL)) + ggtitle(label = &quot;NGA MSNA 2022: average max 5 day rainfall intensity by MSNI&quot;, subtitle = &quot;Higher rainfall intensity is associated with higher MSNI needs&quot;)+ labs(x=&quot;MSNI&quot;, y= &quot;Average max 5 day rainfall intensity for May (mm)&quot;)+ theme_bw() 5.2.5 Shocks nga_svy$data_main |&gt; filter(shock_impact_ability%in% c(&quot;yes&quot;,&quot;no&quot;)) |&gt; group_by(overall_income_estimate,shock_impact_ability) |&gt; summarise( pct_weighted= survey_mean( vartype = &quot;ci&quot;,na.rm=T), n = unweighted(n()) ) |&gt; filter(shock_impact_ability==&quot;yes&quot;) |&gt; ggplot(aes(x=overall_income_estimate, y= pct_weighted))+ geom_point(color=&quot;black&quot;)+ geom_errorbar( aes(ymin = `pct_weighted_low`, ymax = `pct_weighted_upp`), width = 0.2)+ scale_y_continuous( labels = scales::percent )+ labs(x=&quot;Estimated Income (last 30 days)&quot;, title = &quot;HHs who reported shock: Did the impact of shocks cause hunger in your household?&quot;, subtitle = &quot;Northern Nigeria MSNA 2022&quot;, caption = &quot;Approximately 12 % of HHs reported a difficulties/shocks over the last 6 months, but HHs in higher income groups have increased resiliency\\nand ability to absorb shocks and were less affected&quot; )+ coord_flip()+ theme_bw()+ theme( plot.caption = element_text(hjust = 0), axis.title.x = element_blank() ) generally true for both male and female hoh nga_svy$data_main |&gt; filter(shock_impact_ability%in% c(&quot;yes&quot;,&quot;no&quot;)) |&gt; group_by(overall_income_estimate,gender_hoh,shock_impact_ability) |&gt; summarise( pct_weighted= survey_mean( vartype = &quot;ci&quot;,na.rm=T), n = unweighted(n()) ) |&gt; filter(shock_impact_ability==&quot;yes&quot;) |&gt; ggplot(aes(x=overall_income_estimate, y= pct_weighted))+ geom_point(aes(color=gender_hoh))+ geom_line(aes(color=gender_hoh, group=gender_hoh))+ scale_y_continuous( labels = scales::percent )+ labs(x=&quot;Estimated Income (last 30 days)&quot;, title = &quot;HHs who reported shock: Did the impact of shocks cause hunger in your household?&quot;, subtitle = &quot;Northern Nigeria MSNA 2022&quot;, caption = &quot;Approximately 12 % of HHs reported a difficulties/shocks over the last 6 months, but HHs in higher income groups have increased resiliency\\nand ability to absorb shocks and were less affected&quot; )+ coord_flip()+ theme_bw()+ theme( plot.caption = element_text(hjust = 0), axis.title.x = element_blank() ) 5.3 Colombia 5.4 Methodology RS indicators were made an extracted using GEE. This work is contained in the {surveyGEER} repository. Generally the indicators were extracted at the pixel level. Temporal RS data sources with longer timelines were typically preferred over those with shorter time lines so that pixel-values could be normalized to standard scores and indices such as percent mean/median could be constructed. "],["estimating-visualizing-forest-cover-loss.html", "6 Estimating &amp; Visualizing Forest Cover Loss 6.1 Intro 6.2 Load data 6.3 Map 6.4 Extract each years deforestation data for AOI 6.5 Deforestation Results", " 6 Estimating &amp; Visualizing Forest Cover Loss 6.1 Intro This document will show you how to estimate yearly forest cover loss over an area of interest based on the Hansen Global Forest Change Data set (Hansen et al. 2013) library(surveyGEER) library(rgee) library(leaflet) library(sf) library(tidyverse) ee_Initialize() 6.2 Load data load area of interest (aoi) send aoi to ee with rgee::sf_as_ee() load global deforestation data set # local aoi file gdb &lt;- &quot;C:\\\\Users\\\\zack.arno\\\\Documents\\\\sccrap\\\\drc_aoi&quot; layer_name &lt;- &quot;AGORA_RDC_Limite_de_la_chefferie_Tumbwe_RGC_03112022&quot; aoi &lt;- sf::st_read(gdb, layer_name) |&gt; st_transform(crs = 4326) # load to ee aoi_ee &lt;- rgee::sf_as_ee(aoi |&gt; select(NOM)) # global deforestation data set gfc &lt;- ee$Image(&quot;UMD/hansen/global_forest_change_2021_v1_9&quot;) 6.3 Map quick viz based on visualization provided here # set up visualization treeLossVisParam &lt;- list( bands = c(&quot;lossyear&quot;), min = 0, max = 21, palette = c(&quot;yellow&quot;, &quot;red&quot;) ) m1 &lt;- leaflet() |&gt; addProviderTiles(providers$CartoDB.DarkMatter) |&gt; addPolygons(data = aoi, fillOpacity = 0) Map$centerObject(aoi_ee, 15) m1 + Map$addLayer(gfc, treeLossVisParam, &quot;tree loss year&quot;) See 6.1. Figure 6.1: Orange areas are deforestation 6.4 Extract each years deforestation data for AOI # set up named list to iterate through years_of_interest &lt;- c(1:21) |&gt; set_names(2001:2021) # iterate and produce appropriate data.frame def_2001_2021 &lt;- years_of_interest |&gt; purrr::map2_dfr( .y = names(years_of_interest), ~ { def_temp &lt;- gfc$ select(c(&quot;lossyear&quot;))$ eq(.x) def_area &lt;- def_temp$ multiply(ee$Image$pixelArea())$ divide(1e6) def_in_geom &lt;- def_area$reduceRegion( reducer = ee$Reducer$sum(), geometry = aoi_ee, scale = 30, maxPixels = 1e8 ) km2_lost &lt;- def_in_geom$getInfo()$lossyear data.frame( year = .y, km2_lost = km2_lost, hectares_lost = km2_lost * 100 ) } ) 6.5 Deforestation Results view results as Tables 6.2 DT::datatable(def_2001_2021) Figure 6.2: Yearly Deforestation References "],["urban-masking-for-environmental-monitoring.html", "7 Urban Masking for Environmental Monitoring 7.1 Intro &amp; Problem 7.2 Data Sources/Processing 7.3 Solution 7.4 Discussion", " 7 Urban Masking for Environmental Monitoring 7.1 Intro &amp; Problem Urbanized/built up land cover classes pose several unique challenges for remote sensing. In our case they complicate our understanding when attempting to assess vegetative health metrics and link those metrics to communities. In many cases, the communities we are interested in should be considered rural, but HH locations fall on land that has been cleared and built-up (i.e roads, structures, other infrastructure). Problem Statement: We want to use remote sensing to monitor environmental conditions affecting communities/villages of interest. Most commonly we receive point or polygon data representing the population/village/community. Often these spatial data are located on built-up areas. Certain types of environmental/climatic monitoring, like rainfall monitoring, is not affected by built-up land cover types. However, vegetative monitoring/anomaly detection may not make sense over urbanized pixels. Vegetative health of areas directly around the community are likely to have more impact on various living conditions of the community as these are the areas where natural resources are grown/harvested We want to monitor these conditions for thousands of villages so we need a solution that can be scaled up. library(surveyGEER) library(tidyrgee) library(rgee) library(tidyverse) ee_Initialize(quiet = T) 7.2 Data Sources/Processing 7.2.1 Land Use Land Cover There are a variety of global land use land cover data sets available. Some of the most commonly used include: a. Dynamic World (Google)(Brown et al. 2022), b. ESA WorldCover (Zanaga et al. 2021), c. ESRI 2020 (Karra et al. 2021). They have different strengths and weaknesses, but for reasons discussed in Chapter X as well as arguments made by Venteer et al., 2020 (Venter et al. 2022), ESA WorldCover seems like a good starting place for us. Lets look at the ESA World Cover classification for a rural town in Northern Nigeria. If you add satellite imagery to the background (click layers button and check Esri.WorldImagery) it is apparent that the LULC classification does a decent job classifying built-up areas. The rest of the categories are less clearly accurate # set aoi as rural town rural_town &lt;- ee$Geometry$Point(list(5.501897, 13.12175)) # load in landcover esa_ic &lt;- ee$ImageCollection(&quot;ESA/WorldCover/v100&quot;) esa_img &lt;- ee$Image(esa_ic$first())$rename(&quot;esa_lulc_10m&quot;) # grab colors/labels for legend esa_color_table &lt;- ee_landcover_lookup(landcover = &quot;esa&quot;) esa_labels &lt;- esa_color_table$category esa_colors &lt;- esa_color_table$hex # create viz vis_esa &lt;- list( min = 09, max = 110, palette = esa_colors, values = esa_labels, opacity = 0.5 ) # center map Map$centerObject(rural_town, 14) # add layers and legend m_esa &lt;- Map$addLayer(esa_img, vis_esa, &quot;ESA Land Cover&quot;) esa_legend &lt;- Map$addLegend(vis_esa, name = &quot;ESA Legend&quot;, color_mapping = &quot;character&quot;) m_esa_with_leg &lt;- m_esa + esa_legend # visualize m_esa_with_leg Ifwe change the opacity and include a satellite image for the background it is apparent that the LULC classification does a decent job classifying built-up areas. The rest of the categories appear less accurate. 7.2.2 Vegetative Health Indicator Now lets visualize NDVI in the same area. Below we display median NDVI values over the growing season from the MODIS Terra satellite. On top of the NDVI visualization we add just the Urban/Built Up component of the ESA World Cover layer. modis_ic &lt;- ee$ImageCollection(get_modis_link(&quot;terra&quot;)) # mask clouds and scale ndvi modis_ic_masked &lt;- cloud_scale_modis_ndvi(modis_ic, mask = &quot;cloud&amp;quality&quot;) # make tidyee ndvi_tidy &lt;- as_tidyee(modis_ic_masked) # filter ndvi_growing_season &lt;- ndvi_tidy |&gt; filter(date &gt;= &quot;2021-06-20&quot;, date &lt;= &quot;2021-09-26&quot;) ndvi_growing_season_median &lt;- ndvi_growing_season |&gt; summarise(stat = &quot;median&quot;) urban_built &lt;- esa_img$updateMask(esa_img$eq(50)) modis_ndvi_viz &lt;- list( min = 0, max = 1.1, palette = c(&quot;orange&quot;, &quot;yellow&quot;, &quot;green&quot;, &quot;darkgreen&quot;) ) map_ndvi &lt;- Map$addLayer(ndvi_growing_season_median$ee_ob, visParams = modis_ndvi_viz, &quot;ndvi growing&quot; ) map_ndvi_w_urban &lt;- map_ndvi + Map$addLayer( urban_built, list( min = 0, max = 1, palette = &quot;red&quot;, opacity = 0.3 ), &quot;Urban Built&quot; ) map_ndvi_w_urban It is apparent that the pixels falling within the area classified as urban have lower NDVI values than the immediately surrounding areas. This document is static due to API credential limitations, but this difference in pixel values is even more clear when performing the visualization in real time and you can toggle layers on and off. If you look at the satellite basemap above this makes sense. The built-pu area has more bare soil which has a lower NDVI than vegetation. While this is true, if we are interested in studying the conditions of the populations residing in the urban area, the vegetative health in the surrounding area may be more likely to impact there conditions as this is where most of the resources may be grown/harvested. 7.3 Solution Mask the urban area from the NDVI layer below you can see the urban area masked from the NDVI image. The pixel values are set to null. ndvi_urban_masked &lt;- ndvi_growing_season_median$ee_ob$updateMask(esa_img$neq(50)) map_ndvi_masked &lt;- Map$addLayer(ndvi_urban_masked, visParams = modis_ndvi_viz, &quot;NDVI Growing Season (Urban Masked)&quot; ) map_ndvi_masked Next we run a focal statistic (moving window calculation) that reclassifies each pixel based on a provided number of nearest neighbors. In the below visualization we have run a focal_median statistic on the urban-masked NDVI Image using a 3 pixel (750 m) radius. As the urban areas were pre-masked none of the original NDVI values within the urban area were used in the calculation. The urban area where we are going extract our NDVI indicators from has now been filled with values that more closely approximate the areas around the urban zone than previously. ndvi_urban_masked_focal_median &lt;- ndvi_urban_masked$ focal_median( radius = 3, kernelType = &quot;circle&quot;, units = &quot;pixels&quot; )$ reproject(ndvi_tidy$ee_ob$first()$projection()) map_ndvi_focal_median_w_mask &lt;- map_ndvi + Map$addLayer( ndvi_urban_masked_focal_median, visParams = modis_ndvi_viz, &quot;NDVI Focal Median&quot; ) + Map$addLayer( urban_built, list( min = 0, max = 1, palette = &quot;red&quot;, opacity = 0.2 ), &quot;Urban Built&quot; ) 7.4 Discussion The main subjective decisions in this process are within regards to the focal statistic: kernelType , focal statistic, and radius can all be adjusted for different reasons. For our purposes radius will most likely be the most important consideration. A larger radius will include more pixels in focal statistic and therefore create a smoother surface. References "],["graphing-rainy-season-delay-compared-to-historical.html", "8 Graphing Rainy Season Delay Compared to Historical 8.1 Libraries &amp; Data 8.2 Temporally filter data 8.3 Calculate cumulative precipitation 8.4 Discussion 8.5 References", " 8 Graphing Rainy Season Delay Compared to Historical This chapter will show some useful ways to extract and visualize rainfall data to a specific region. Here we will look look at the district of Turkana in Norther Kenya. However, we the user could supply any sf POLYGON of interest. Once the data is extracted to the zone we will transform the daily chirps rainfall data to weekly cumulative over the course of the year. We will do this for the year 2022 and compare it to the historical averages to get a sense of rainfall timing. 8.1 Libraries &amp; Data library(surveyGEER) library(tidyverse) library(tidyrgee) library(rgee) library(sf) library(lubridate) ee_Initialize() load chirps daily image collection and convert to tidyee format We use {rgeoboundaries} (Dicko 2022) to extract a specific admin 2 in KEN (Turkana). However if you have a local file you can also just load it using the {sf} package (Pebesma 2021) (st_read()) cast POLYGON in case there are any MULTIPOLYGONs that could cause issues select the minimal number of columns needed to identify the polygon you are extracting to. GEE has issues with certain column types like LOGICAL, so its good to trim it down here. extract data with tidyrgee::ee_extract_tidy() which has some convenience processes to make the data return in a user-friendly format (Arno and Erickson 2023) chirps &lt;- ee$ImageCollection(&quot;UCSB-CHG/CHIRPS/DAILY&quot;) # bring in tidyrgee tidyee format chirps_tidy &lt;- as_tidyee(chirps) ken_adm2 &lt;- rgeoboundaries::geoboundaries(c(&quot;Kenya&quot;), adm_lvl = 2) turkana &lt;- ken_adm2 |&gt; filter(str_detect(shapeName, &quot;TURK&quot;)) # cast to single poly turkana_poly &lt;- st_cast(turkana, &quot;POLYGON&quot;) # simplify file/rm col names turkana_poly &lt;- turkana_poly |&gt; select(shapeName) # extraction - this part takes the longest chirps_daily_turkana &lt;- chirps_tidy |&gt; ee_extract_tidy(y = turkana_poly, stat = &quot;median&quot;, scale = 5500, via = &quot;drive&quot;) 8.2 Temporally filter data set up current data set up baseline data chirps_daily_turkana &lt;- chirps_daily_turkana |&gt; mutate( year = year(date), month = month(date), week = week(date) ) chirps_turkana2022 &lt;- chirps_daily_turkana |&gt; filter( year == 2022 ) chirps_historical_turkana &lt;- chirps_daily_turkana |&gt; filter(year &lt;= 2021) 8.3 Calculate cumulative precipitation calculate cumulative weekly precipitation for historical baseline data do the same for current data (2022 in this example) Note: when we group_by(year,week) we loose the date. Although the date is not completely necessary as we could plot weeks on the x-axis, it is convenient to have the date and I think it makes the plot easier to read. Therefore we add it back in using the {ISOWeek} package (Block and Hatto von Hatzfeld 2011) Bind the historical and current data.frames together for graphing historical_precip_weekly_accumulation &lt;- chirps_historical_turkana |&gt; group_by(year, week) |&gt; summarise( weekly_precip = sum(value) ) |&gt; mutate( cumulative_precip = cumsum(weekly_precip) ) |&gt; group_by(week) |&gt; summarise( cumulative_precip = mean(cumulative_precip), ) |&gt; mutate( date = ISOweek::ISOweek2date(paste0(&quot;2022&quot;, &quot;-W&quot;, formatC(week, width = 2, flag = &quot;0&quot;), &quot;-1&quot;)), time_frame = &quot;historical&quot; ) weekly_cumulative_precip_2022 &lt;- chirps_turkana2022 |&gt; group_by(week) |&gt; summarise( weekly_precip = sum(value) ) |&gt; mutate( cumulative_precip = cumsum(weekly_precip) ) |&gt; group_by(week) |&gt; summarise(cumulative_precip = max(cumulative_precip)) |&gt; select(week, cumulative_precip) |&gt; mutate( time_frame = &quot;2022&quot;, date = ISOweek::ISOweek2date(paste0(&quot;2022&quot;, &quot;-W&quot;, formatC(week, width = 2, flag = &quot;0&quot;), &quot;-1&quot;)) ) historical_current &lt;- bind_rows(weekly_cumulative_precip_2022, historical_precip_weekly_accumulation) With some ggplot  we can visualize the cumulative precipitation for both the current/year of interest and the historical time period in Figure 8.1. historical_current |&gt; ggplot(aes( x = date, y = cumulative_precip, color = time_frame, group = time_frame )) + annotate(&quot;rect&quot;, xmin = lubridate::ymd(c(&quot;2022-03-01&quot;)), xmax = lubridate::ymd(c(&quot;2022-05-31&quot;)), ymin = 0, ymax = Inf, alpha = 0.2, fill = &quot;red&quot; ) + annotate(&quot;rect&quot;, xmin = lubridate::ymd(c(&quot;2022-10-01&quot;)), xmax = lubridate::ymd(c(&quot;2022-12-31&quot;)), ymin = 0, ymax = Inf, alpha = 0.2, fill = &quot;red&quot; ) + geom_path() + scale_x_date( date_breaks = &quot;1 month&quot;, date_labels = &quot;%b&quot; ) + labs(y = &quot;Cummulative Precipitation&quot;) + ggtitle(&quot;Turkana Cumulative Precipiation&quot;, subtitle = &quot;MAM &amp; OND Seasons Highlighted&quot; ) + theme_bw() + theme( text = element_text(size = 10) ) Figure 8.1: Cumulative precip graph provide a nice way to look at timing of rainfall in Figure 8.2 we zoom in further to just look at the MAM season. historical_current |&gt; filter(date &gt;= &quot;2022-03-01&quot;, date &lt;= &quot;2022-05-31&quot;) |&gt; ggplot(aes( x = date, y = cumulative_precip, color = time_frame, group = time_frame )) + annotate(&quot;rect&quot;, xmin = lubridate::ymd(c(&quot;2022-03-01&quot;)), xmax = lubridate::ymd(c(&quot;2022-05-31&quot;)), ymin = 0, ymax = Inf, alpha = 0.2, fill = &quot;red&quot; ) + geom_path() + scale_x_date( date_breaks = &quot;1 month&quot;, minor_breaks = &quot;1 day&quot;, date_labels = &quot;%b&quot; ) + labs(y = &quot;Cummulative Precipitation&quot;) + ggtitle(&quot;Turkana Cumulative Precipiation&quot;, subtitle = &quot;MAM Season&quot; ) + theme_bw() + theme(axis.text.x = element_text(angle = 90)) Figure 8.2: If we zoom in further the differenc in start of season rains becomes more apparent 8.4 Discussion TBD 8.5 References References "],["threshold-based-surface-water-classification---cautionary.html", "9 Threshold Based Surface Water Classification - cautionary Take aways 9.1 Setup AOI 9.2 LandSat 8 Pre-Processing 9.3 Band Math and thresholding 9.4 Vizualize Results 9.5 Time series 9.6 Conclusion/MNDWI", " 9 Threshold Based Surface Water Classification - cautionary In certain environments it is possible to obtain a decent water mask from simple band math and threshold techniques. Here we explore the MNDWImetric in the context of a reservoir in Iraq. Partners were interested in understanding the rervoir levels/conditions specifically in April 2022 with respect to previous years at the same time of year. Take aways MNDWI thresholding may provide a decent qualitative understanding of conditions, but we do not recommend putting much weight into quantitative analysis of the resulting water mask. Comparing snapshot of periods of interest can be misleading and more thorough temporal analysis should be done to better understand the surface water dynamics We do observe artefacts from this technique - therefore manual interpretation of results in necessary prior to any conclusions or subsequent analysis or narrative is formed. It is better to use pre-existing surface water layers such as JRC where possible. The difficulty with JRC is that it can be out of date. In these cases more advanced techniques are recommended While this case study is more a word of caution, we can still use it to learn some cool techniques. Here we look at some threshold maps for water classification. We classify surface water using this technique for a specific month each year. 9.1 Setup AOI library(surveyGEER) library(rgee) library(tidyverse) library(sf) library(tidyrgee) ee_Initialize() geom &lt;- ee$Geometry$Polygon(list( c(44.2354847930793, 34.83077069846819), c(44.261577322376176, 34.692001577255105), c(44.40851946104805, 34.511140220037795), c(44.607646658313676, 34.47152432533435), c(44.687297537219926, 34.58918498051208), c(44.5211293243293, 34.75069665768057), c(44.393413259876176, 34.810477595189816), c(44.28217668761055, 34.88598770009403), c(44.223125173938676, 34.84316957807562) )) Map$centerObject(geom, 10) map_irq_reservoir &lt;- Map$addLayer(eeObject = geom, name = &quot;geom&quot;) map_irq_reservoir 9.2 LandSat 8 Pre-Processing filter temporally and spatially to area of interest remove images with &gt;= 25% cloud cover rename bands l8_ic &lt;- ee$ImageCollection(&quot;LANDSAT/LC08/C02/T1_L2&quot;)$ filterDate(&quot;2013-01-01&quot;, &quot;2022-12-31&quot;)$ filterBounds(geom)$ filter(ee$Filter$lt(&quot;CLOUD_COVER&quot;, 25))$ select( list(&quot;SR_B2&quot;, &quot;SR_B3&quot;, &quot;SR_B4&quot;, &quot;SR_B5&quot;, &quot;SR_B6&quot;, &quot;SR_B7&quot;), list(&quot;blue&quot;, &quot;green&quot;, &quot;red&quot;, &quot;nir&quot;, &quot;swir1&quot;, &quot;swir2&quot;) ) Convert to tidy format for easy monthly composites Filter to just April months l8_ic_tidy &lt;- as_tidyee(l8_ic) l8_median_april &lt;- l8_ic_tidy |&gt; group_by(year, month) |&gt; summarise( stat = &quot;median&quot; ) |&gt; ungroup() |&gt; filter(month == 4) 9.3 Band Math and thresholding calculate the normalized difference of green &amp; swir1 bands calcualte the normalied diffference of green &amp; swir2 bands l8_median_april_mndwi1 &lt;- l8_median_april$ee_ob$map( function(img) { time_start &lt;- img$get(&quot;system:time_start&quot;) img_mndwi &lt;- img$normalizedDifference(list(&quot;green_median&quot;, &quot;swir1_median&quot;))$ rename(&quot;mndw1&quot;) return(img_mndwi$set(&quot;system:time_start&quot;, time_start)) } ) l8_median_april_mndwi2 &lt;- l8_median_april$ee_ob$map( function(img) { time_start &lt;- img$get(&quot;system:time_start&quot;) img_mndwi &lt;- img$normalizedDifference(list(&quot;green_median&quot;, &quot;swir2_median&quot;))$ rename(&quot;mndw2&quot;) return(img_mndwi$set(&quot;system:time_start&quot;, time_start)) } ) Convert back to tidyee class for easy joinning Now we have an IC with 2 bands (mndwi1, mndwi2) we can create the water mask by thresholding on both mndwi indices. mndwi_tidy &lt;- as_tidyee(l8_median_april_mndwi1) mndwi2_tidy &lt;- as_tidyee(l8_median_april_mndwi2) mndwi_tidy &lt;- mndwi_tidy |&gt; inner_join(mndwi2_tidy, by = &quot;system:time_start&quot;) water_yearly &lt;- mndwi_tidy$ee_ob$map( function(img) { time_start &lt;- img$get(&quot;system:time_start&quot;) img$ select(&quot;mndw1&quot;)$gte(0)$ And(img$select(&quot;mndw2&quot;)$gte(0))$ set(&quot;system:time_start&quot;, time_start)$rename(&quot;water&quot;) } ) water_yearly_tidy &lt;- as_tidyee(water_yearly) 9.4 Vizualize Results It is critical to visualize the results and explore them maually. Here we create a leaflet map and use R and {purrr} to iterate and add and label each year as a layer to the map. This can be done GEE javascript, but for R users this may be simpler and a good example of the power of combining languages with the {rgee} app. In an interactive session you would have all the layers in one map which you could toggle on and off. As this is a static document. We will just display 3 selected maps. years_to_map &lt;- 2013:2022 years_to_map &lt;- years_to_map |&gt; set_names(paste0(years_to_map, &quot;_water&quot;)) yearly_water_maps &lt;- years_to_map |&gt; purrr::map2( .y = names(years_to_map), ~ Map$addLayer( water_yearly_tidy |&gt; filter(year == .x) |&gt; as_ee(), visParams = list(min = 0, max = 1, palette = c(&quot;white&quot;, &quot;blue&quot;)), name = .y ) ) Map$centerObject(geom, 10) yearly_water_maps$`2013_water` + yearly_water_maps$`2014_water` + yearly_water_maps$`2015_water` + yearly_water_maps$`2016_water` + yearly_water_maps$`2017_water` + yearly_water_maps$`2018_water` + yearly_water_maps$`2019_water` + yearly_water_maps$`2020_water` + yearly_water_maps$`2021_water` + yearly_water_maps$`2022_water` + Map$addLayer(geom, {}) Figure 9.1 shows the water mask for 2013 Figure 9.1: water mask 2013 In Figure 9.2 we display the water mask for 2017. Here we see some artefacts that look to be negatively affecting our classification. Figure 9.2: 2017 water mask appears to be affected by artefacts and should be investigated furhter And finally the water classification for April 2022 (Figure 9.3 ) Figure 9.3: mndwi derived water water mask for april 2022 The output shown in Figure 9.2 clearly indicates further investigation into the water classificaition is warranted. Nonetheless, for the sake of showing some more {rgee} &amp; {tidyrgee} tricks we will perform some analysis for the AOI. 9.5 Time series ee_extract_tidy() is a wrapper for rgee::ee_extract() which offers a little bit more flexibility on the inputs as well as ensures a consistent and tidy returned output. Here we use it to extract the the total number of pixels classified as water in the AOI. Note: It is important to include the correct scale of the image. water_pixels_in_geom &lt;- water_yearly_tidy |&gt; ee_extract_tidy(y = geom, scale = 30, stat = &quot;sum&quot;) Now that we have the data in a nice tidy tabular data.frame we can easily apply dplyr and ggplot to manipulate and visualize the data. water_pixels_in_geom |&gt; mutate(area_km2 = (value * 30 * 30) * 1e-6) |&gt; ggplot(aes(x = date, y = area_km2)) + geom_line() + geom_point() + scale_x_date(date_breaks = &quot;1 year&quot;, date_labels = &quot;%Y&quot;) + labs(x = &quot;Date&quot;, y = &quot;Area (Km2)&quot;, title = &quot;Approximate Reservoir Area&quot;) + theme_bw() 9.6 Conclusion/MNDWI the use of MNDWI based thresholding for water classification should generally be discouraged. JRC open source layers and more advanced methods (insert link) should be preferred. Additionally comparing 2 snap shot of time is also discouraged as conclusions drawn from this comparison will generally be misleading. Nonetheless, this technique can be used for a quick initial qualitative assessment. By demonstrating this process we hope to document these takeaways while demonstrating some {rgee} and {tidyrgee} applications. "],["monitoring-early-onset-crises-with-ntl.html", "10 Monitoring Early Onset Crises with NTL 10.1 Downloading 10.2 Processing 10.3 Use-Case Examples", " 10 Monitoring Early Onset Crises with NTL At the onset of war in Ukraine it was difficult to obtain accurate information regarding the conditions on the ground. Therefore, a study of Night Time Light luminosity was conducted. The analyses revealed significant shifts in night-time lights over the coarse of the invasion. These changes can be attributed to: damage to infrastructure change in human activity connected to norms established through martial law change in human activity connected to the ongoing evacuations and relocation of residents to other parts of the country or neighboring cities Night time lights data can be obtained from several sources, but the blackmarble VNP46A suite images was required for this work to obtain and deliver analyses in near real time (NRT) 10.1 Downloading The NASA - LAADS WorldView Portal was used to query and download raster images. The Black Marble Nighttime Blue/Yellow Composite (DNB) which combines the Black Marble TOA nighttime radiance product with infrared bands was essential for querying and filtering the images to detect influence of moonlight and clouds. An excellent article by Joseph M. Smith was used as the basis of this technique. Use blue-yellow composite imagery to better understand cloud formations &amp; atmospheric effects If downloading data from the last ~ 72 hours you will need to DL from the NRT collection It is possible that you will get a message saying the imagery from the NRT collection is not available. If this happens it is because you are somehow logged into LAADs system, but not the NRT catalog. In this case go to the NRT Portal and click log in. To facilitate the processing and reprojecting of .h5 files obtained from the world view portal the {blackmarble} R package was developed based on script provided by NASA The development version of blackmaRble can be downloaded from GitHub with: # install.packages(&quot;devtools&quot;) devtools::install_github(&quot;zackarno/blackmaRble&quot;) After filtering images save all the .h5 format raster tiles to 1 directory with the default NASA file name syntax. 10.2 Processing library(tidyverse) # devtools::install_github(&quot;zackarno/blackmaRble&quot;) library(blackmaRble) library(here) library(terra) write_rasters &lt;- c(T,F)[1] # toggle 1 for write outputs, 2 for no outputs 10.2.1 Reprojecting using list of file names &amp; list of of full path names we can read in all the files in the folder and reproject them all at once results are saved in list object: ntl_reprojected # dir.create(&quot;ntl_h5&quot;) h5_filenames &lt;- list.files(here::here(&quot;ntl_h5/&quot;),pattern = &quot;\\\\.h5$&quot;) h5_paths &lt;- list.files(here::here(&quot;ntl_h5/&quot;), full.names = T,pattern = &quot;\\\\.h5$&quot;) ntl_reprojected &lt;- h5_paths |&gt; map(~vnp46A_to_wgs84(path =.x)) |&gt; set_names(h5_filenames) 10.2.2 Subset bands to simplify list object lets just subset each item in list to the DNB band new object dnb_radiance contains only DNB_At_Sensor_Radiance_500m band for each item in list. # subset to just main band of interest for faster mosaicing dnb_radiance &lt;- ntl_reprojected |&gt; map(~.x |&gt; subset(&quot;DNB_At_Sensor_Radiance_500m&quot;)) 10.2.3 Mosaic &amp; write rasters since study area is composed of 4 tiles we need to mosaic them together function below uses NASA naming syntax to group tiles together by DOY and then mosaic them specify output to automatically write as tiff to specified directory result is also saved as spatRaster collection # dir.create(&quot;ntl_doy_tiffs&quot;) if(write_rasters){ ntl_rad_by_doy &lt;- mosaic_vnp46a_by_doy(dnb_radiance,output = here::here(&quot;ntl_doy_tiffs&quot;)) } # ntl_rad_by_doy$`084` 10.3 Use-Case Examples Impact-Initiatives Ukraine Night-time Lights Pre and Post Conflict Escalation - February vs March 2022 "],["hex-maps.html", "11 Hex Maps 11.1 NDVI Hex Maps 11.2 Chirps Hex Maps", " 11 Hex Maps Here we present a tutorial to create hexagon aggregation maps similar to the two below: NDVI Standard Score Map Precipitaiton Record Low Map 11.1 NDVI Hex Maps 11.1.1 Inputs/Set up Data First we load the required libraries and modis + chirps image collections using {rgee} library(surveyGEER) library(rgee) library(tidyrgee) library(sf) library(tidyverse) library(leaflet) ee_Initialize() modis_link &lt;- &quot;MODIS/006/MOD13Q1&quot; modisIC &lt;- ee$ImageCollection(modis_link) 11.1.2 Generate Grid We will use some classics from the {sf} to generate a grid. Generally, you would have a polygon file that you would likely load with sf::st_read() and then use to generate a grid over. If you are following along and and have a file you are interested in using go ahead and do that. If not, you can follow along with Somalia boundary file that is built into the {surveyGEER} and loaded automatically when library(surveyGEER) is called. first lets quickly view the file som_boundary ## Simple feature collection with 1 feature and 0 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 40.98863 ymin: -1.654238 xmax: 51.41515 ymax: 11.98836 ## Geodetic CRS: WGS 84 ## geom ## 1 MULTIPOLYGON (((43.47679 11... we reproject the data to utm +{surveyGEER} contains some convenience functions for re-projecting the data so you dont have to remember any coordinate system names or epsg codes. Checkout ?reach_reproject_utm() for more details. We make a grid over the file. sf::st_make_grid creates a grid over the entire bounding box of the x input. When the data is in UTM the cellsize is meters. Here we arbitrarily choose a grid diameter of 50 km. set square to F to create a hexagon grid. clip grid to boundary file To improve efficiency and reduce processing time we want to minimize the number of grid cells that we later have to perform zonal statistics/reductions on. For some reason the attribute where the spatial information was being stored was renamed to x. Lets rename it to geometry - pro-tip: this is a common gotcha with {rgee} - the geometry column needs to be named geometry som_boundary_utm &lt;- som_boundary |&gt; reach_reproject_utm(country_code = &quot;som&quot;) grid_utm &lt;- st_make_grid(x = som_boundary_utm, cellsize = 15000,square=F) |&gt; st_as_sf() grid_utm_clipped &lt;- grid_utm[som_boundary_utm,] |&gt; rename(geometry=&quot;x&quot;) Always good visualize the results of our code. Lets map the grid to make sure it is right. We will use {leaflet} which requires the data to be in the coordinate reference system of wgs 84. Therefore we will re project the data prior to mapping it in Figure 11.1 grid_wgs84_clipped &lt;- st_transform(grid_utm_clipped,crs = 4326) leaflet(grid_wgs84_clipped ) |&gt; addTiles() |&gt; addPolygons() Figure 11.1: 50 km hexagon diameter grid generated for SOM 11.1.3 Image Collection Processing MODIS NDVI should be transformed by multiplying the raster values by 0.0001, below you can get a sense of what typical GEE/RGEE syntax looks like. For must R-users this looks a bit strange. tidyrgee which is shown in the following chunks simplifies a great deal of this for the end-user. Since raster math has not yet been added to to tidyrgee (coming soon), we will leave the the classical syntax below. modis_ndvi &lt;- modisIC$ select(&quot;NDVI&quot;)$ map( ee_utils_pyfunc( function(x){x$ multiply(0.0001)$ copyProperties(x,x$propertyNames()) } ) ) Now its time for some actual analysis. Here is where tidyrgee shines and vastly simplifies the code. To use the full power of tidyrgee you should convert the imageCollection to a tidyee class object with the simple function as_tidyee. Once this has been performed it is very easy to group, filter, composite(summarise) the imageCollection. To calculate a standard score (Z-score) for monthly data we first need to create a baseline. Therefore we filter the imageCollection to 2000-2015. Then we group that data by month and summarise, calculating a mean and standard deviation for each month. The result will store an imageCollection of 12 images (1 per month). Each image will have 2 bands the NDVI_mean and NDVI_sd. Statistics are calculated at the pixel-level for each image. modis_ndvi_tidy &lt;- as_tidyee(modis_ndvi) monthly_baseline &lt;- modis_ndvi_tidy |&gt; filter(year %in% 2000:2015) |&gt; group_by(month) |&gt; summarise(stat=list(&quot;mean&quot;,&quot;sd&quot;)) Now that we have our baseline lets calculate the the average monthly NDVI for more recent years so that we can compare. We will first filter the ImageCollection to 2016-2022. Since we are working with the MODIS NDVI 16-day composite we have approximately 2 images per month. Therefore to simplify the process we should take the average NDVI value (per pixel) each month. To do this we can first group_by(year,month) and then call summarise. Remember if we just called group_by(month) it would return 12 images with 1 monthly average value for the entire time period of 2022-2012. By grouping by year and month we get the average monthly value for each year! ndvi_recent_monthly &lt;- modis_ndvi_tidy |&gt; filter(year %in% c(2016:2022)) |&gt; group_by(year,month) |&gt; summarise( stat=&quot;mean&quot; ) Since I know I want to eventually combine the baseline stats calculated previously (NDVI_mean, NDVI_sd) with these new monthly means ,I need to rename the band produced in this new summary. By default the band_name is written as: original band names + _ + stat. To avoid duplicate band names later when joining I will rename the band names in ndvi_recent_monthly from NDVI_mean just to NDVI. We can do this with the select method just by supplying a name (will be adding a pure rename function soon). ndvi_recent_renamed &lt;- ndvi_recent_monthly |&gt; select(NDVI=&quot;NDVI_mean&quot;) Now we are ready to join the ImageCollection bands into 1 ImageCollection. The following inner join returns 1 ImageCollection storing the monthly average NDVI values from 2016-2022 and the long-term (baseline) monthly average and standard deviations of NDVI ndvi_recent_and_baseline&lt;- inner_join(x = ndvi_recent_renamed, y = monthly_baseline, by = &quot;month&quot;) Now we have all the necessary ingredients to calculate a standard score (Z-score). Below you get another opportunity to view some of trickier rgee/GEE syntax. This expression functionality has not yet been adapted/wrapped in tidyrgee (coming soon) so using the rgee syntax is still our only option. This is why a basic understanding of GEE and rgee is still quite handy. rgee cannot handle tidyee classes, therefore we need to convert the object back to the ee$ImageCollection format rgee was written for. Luckily tidyrgee is being developed to be inter-operable with rgee and provides simple helper functions such as as_ee for this type of conversion. After running as_ee we have access to all the methods available in rgee/GEE. ndvi_recent_baseline_imageCol &lt;- ndvi_recent_and_baseline |&gt; as_ee() ndvi_recent_baseline_imageCol$first()$bandNames()$getInfo() ndvi_zscore&lt;- ndvi_recent_baseline_imageCol$map( function(img){ zscore&lt;- img$expression( &quot;float((NDVI-NDVI_mean)/(NDVI_sd))&quot;, opt_map= list(NDVI= img$select(&quot;NDVI&quot;), NDVI_mean= img$select(&quot;NDVI_mean&quot;), NDVI_sd= img$select(&quot;NDVI_sd&quot;) ) )$rename(&quot;NDVI_z_score&quot;) img$select(&quot;NDVI&quot;,&quot;NDVI_mean&quot;)$addBands(zscore) } ) 11.1.4 Zonal stats per hexagon ndvi_zscore is now an ee$ImageCollection class object. We can easily convert back to tidyee with as_tidyee(). However, the zonal statistics tool ee_extract_tidy() works on both on ee$ImageCollection and tidyee. Just for ease of filtering and selecting lets convert it back to tidyee Zonal statistics are heavery computationally expensive calculations. For example I ran the stats to calculate the median value of all 3 bands for every hexagon grid () for every month in the current composite collection (76 images) and it to 160 minutes. Therefore, lets do some pre-processing so we only extract exactly what we need: we are only really interested in the z_score band so lets select that we are only really interested 2021 onwards so lets filter the object to everything from 2021 onwards ndvi_z&lt;- as_tidyee(ndvi_zscore) ndvi_z_pre_processed &lt;- ndvi_z |&gt; filter(year&gt;=2021) |&gt; select(&quot;NDVI_z_score&quot;) You should run ?ee_extract_tidy to check all the optional parameters. It is important to include the scale of the image/imageCollection. If you are using a large featureCollection or sf spatial object you will need to adjust the via argument. I usually have best luck with via=\"drive\". The amount of time to perform an extraction can vary due to computations in the cloud, you must be patient.. If your grid covers the bounding box of your study area, it is probably a good idea to clip it to the area of study polygon to avoid unnecessary computations and save time. grid_with_ndvi &lt;- ndvi_z_pre_processed |&gt; ee_extract_tidy(y = grid_wgs84_clipped,stat = &quot;mean&quot;,scale = 250,via = &quot;drive&quot;) 11.2 Chirps Hex Maps Now lets prep the chirps data for extraction. First convert it to tidyee format so we can leverage the easy filter/summarise methods 11.2.1 Load and composite Image Collection chirps_link &lt;- &quot;UCSB-CHG/CHIRPS/DAILY&quot; chirps &lt;- ee$ImageCollection(chirps_link) chirps_tidy &lt;- as_tidyee(chirps) Lets take the entire daily record from 1981-current and extract the yearly precipitation for each year at the pixel level. Since each pixel represents daily rainfall (mm) we can just group the images by year and sum them to get annual rainfall. chirps_annual_precip &lt;- chirps_tidy |&gt; select(&quot;precipitation&quot;) |&gt; group_by(year) |&gt; summarise( stat=&quot;sum&quot; ) 11.2.2 Zonal Reductions Next lets again use the ee_extract_tidy function to extract these yearly values. Conceptually we are using our grid to perform zonal statistics on each image (1 per year). the output is a long format tidy data frame with all the median yearly values (1 per hex per year). This took about: yearly_rainfall_hex &lt;- chirps_annual_precip |&gt; ee_extract_tidy(y = grid_wgs84_clipped, stat = &quot;median&quot;, scale = 5500, via = &quot;drive&quot;) 11.2.3 Data Wrangling Now that we have a nice clean tidy data.frame we can go nuts with dplyr: We calculate the record lows (getting the 2 lowest values) for each hexagon excluding the year 2022 we calculate how much lower the record was from the next lowest value Then filter to only record breaking rows that occur in 2021 record_lows_2021 &lt;- yearly_rainfall_hex |&gt; # filter out 2022 since it is not yet complete filter(lubridate::year(date)!=2022) |&gt; # group by grid uid group_by(uid) |&gt; # grab two lowest annual records slice_min(n = 2,order_by = value) |&gt; # calculate the difference in the two lowest annual values # and specify which is the lowest with T/F mutate( lower_by = max(value)- min(value), record_low = value==min(value) ) |&gt; ungroup() |&gt; # filter just the lowest record filter(record_low) |&gt; mutate( date= year(date) ) |&gt; #now filter just to year of interest 2021 to # see where that year had all time lowest annual value on record filter(date==2021) We then join the 2021 record breaking records to the original clipped grid and take the centroids of all grid cells write output as shapefile for vizualization in QGIS. # join record lows to original hex object by &quot;uid&quot; to make spatial again # then filter out any hex that does not have record breaking value # change remaining hex&#39;s to centroid points so that we can map as proportional bubbles. record_low_centroids&lt;- grid_wgs84_clipped |&gt; dplyr::left_join(record_lows_2021) |&gt; filter(record_low==T) |&gt; st_centroid() if(!is.null(write_outputs_to)){ sf::st_write(obj = record_low_centroid,dsn = record_low_centroids,layer = &quot;precip_record_low.shp&quot;) } 11.2.4 QGIS Templates We have provided QGIS templates in the map_templates folder of this directory. Once the data has been extracted and save to the data folder in the project you should open up the either of the map templates. To use the map templates you must link the data sources to the layers. Once linked the symbology will automatically be set. To use these templates you should also have some layers of your own. If you put all of you GIS layers into the data folder when you link one the rest will auto-link. You should put the following layers into the data folder: Admin 1 boundary (usually OCHA or UNDP) Admin 2 boundary (usually OCHA or UNDP) Hexagon grid (either pre-existing or created by you above) NDVI layer (Hexagon output by script above) Precipitation layer (point file output by script above) World Adm0 boundaries - downloaded from natural earth knitr::include_graphics(&quot;man/figures/qgis_layers_panel.png&quot;) knitr::include_graphics(&quot;man/figures/qgis_layers_panel.png&quot;) knitr::opts_chunk$set( echo = T, warning = F, message = F, eval=F, fig.fullwidth = TRUE, out.width = &quot;100%&quot; ) "],["urban-growth-somalia.html", "12 Urban Growth Somalia 12.1 Set Up", " 12 Urban Growth Somalia I have not yet had time to translate this into R-code, but here is some python code adapted from the GEE Introduction to Dynamic World (Part 3) - Exploring Time Series(Brown et al. 2022). This can be entered directly into pyqgis inside QGIS. Here I have split up the code into chunks for annotation, but when copyin into pyqgis you should copy all the code into one script: 12.1 Set Up import ee and ee_plugin libraries load Google Dynamic World as dw and select the built band create and AOI geometry define the years of interest for comparison beforeStart, beforeEnd, afterStart, afterEnd are all created grammatically from the years of interest the dynamic world (dw) Image Collection is then filtered and a composite creating using the mean pixel statistic based on these dates import ee from ee_plugin import Map dw = ee.ImageCollection(&#39;GOOGLE/DYNAMICWORLD/V1&#39;).select(&#39;built&#39;) geometry = ee.Geometry.Polygon( [[[42.64822546374793, 3.6619838709757198], [42.64822546374793, 1.5219665932805257], [46.14187780749793, 1.5219665932805257], [46.14187780749793, 3.6619838709757198]]]) beforeYear = 2016 afterYear = 2021 beforeStart = ee.Date.fromYMD(beforeYear, 1, 1) beforeEnd = beforeStart.advance(1, &#39;year&#39;) afterStart = ee.Date.fromYMD(afterYear, 1, 1) afterEnd = afterStart.advance(1, &#39;year&#39;) beforeDw = dw.filterDate(beforeStart, beforeEnd).mean() afterDw = dw.filterDate(afterStart, afterEnd).mean() The built band give probability values that a pixel is urban. Therefore we can set thresholds to classify the pixel as urban or not. In this example we say a probability less that 0.2 is not urban and a probability greater than 0.5 is urban. This thresholds are just used for illustrative purposes and defining an accurate thresholds is a more complex process. However, using these thresholds we can: classify pixels that are urban in the after (2022) Image by masking pixels &gt; 0.5 classify newly urbanized (newUrban) pixels by finding any area where the before date was less than 0.2 and the build band of the after Image is greater than 0.5. # Select all pixels that are # &lt; 0.2 &#39;built&#39; probability before # &gt; 0.5 &#39;built&#39; probability after newUrban = beforeDw.lt(0.2).And(afterDw.gt(0.5)) overlap_urban =afterDw.gt(0.5) set the visualization parameters and add them to the map changeVisParams = {&quot;min&quot;: 0, &quot;max&quot;: 1, &quot;palette&quot;: [&#39;white&#39;, &#39;red&#39;]} urbanVisParams = {&quot;min&quot;: 0, &quot;max&quot;: 1, &quot;palette&quot;: [&#39;white&#39;, &#39;yellow&#39;]} Map.addLayer(newUrban.selfMask().clip(geometry), changeVisParams, &#39;New Urban (Masked)&#39;) Map.addLayer(overlap_urban.selfMask().clip(geometry), urbanVisParams, &#39;Urban (Masked)&#39;) Using these two layers we can easily make the maps below. Figure 12.1 shows urban area in Mogadishu with orange color representing areas that have been urbanized since 2016. Figure 12.1: Urban growth detected in Mogadishu Somalia from 2016 to 2022 Figure 12.2 shows the same map, but of Baidoa Somalia. Figure 12.2: Urban growth detected in Baidoa Somalia from 2016 to 2022 References "],["references-1.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
